---
title: "Report Exercise 2"
author: "Matthias Sedlmayr"
date: "2025-12-20"
output: html_document
---

```{r setup, message=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)
library(here)
```
# Paper Questions\
### Q1\
Explain the difference between a random cross-validation and a spatial cross-validation.\

### Answer:\
In random cross-validation, observations are randomly assigned to training and test folds regardless of their spatial location. The training and test samples are frequently geographically close and environmentally similar. This can lead to overly optimistic estimates of model performance when reference data is spatially clustered because the model is mostly tested under interpolation conditions.\

Spatial cross-validation, separates the training and test data in geographic space. Test locations are spatially distant from training locations, which mimics the situation of making predictions in new areas. This approach more accurately reflects the challenges of spatial upscaling and typically yields lower, but more realistic, performance estimates. However, if the spatial separation is too strong relative to the intended prediction task, spatial cross-validation can underestimate the achievable map accuracy. (Ludwig et al., 2023)\

### Q2\
### Answer\
Do you see an alternative to measuring distance that considers the task of spatial upscaling based on environmental covariates more directly?\

Yes, instead of using geographic (Euclidean) distance, distance should be measured in environmental (feature) space based on the dissimilarity of predictor variables.\

When it comes to spatial upscaling, the difficulty of making a prediction is driven by how different the environmental conditions at the location where the prediction is being made are from those represented in the training data, rather than by physical distance alone. Measuring distance in predictor space directly captures extrapolation risks due to unseen combinations of \

-covariates.\

-unseen combinations of covariate\

-gaps in environmental coverage\

-non-analogue conditions \

Ludwig et al. operationalise this idea through:\

-feature-space cross-validation, which partitions the data into folds that differ in environmental covariate space rather than geographic space and\

-the area of applicability (AOA) concept, which restricts predictions to regions where environmental dissimilarity remains within the range observed during model training.\

These approaches align the validation strategy with the actual goal of spatial upscaling based on environmental covariates and provide a more meaningful assessment of model transferability and map reliability. (Ludwig et al., 2023)\

## Aim 
This exercise aims to assess the impact of different cross-validation strategies (random, spatial and environmental) on the apparent predictive performance of a random forest model for predicting leaf nitrogen concentration leafN. \

## Data

```{r data-prep}
load(here::here("data/leafN_prepared_data.RData"))
skimr::skim(dfs)
```
## Methods
A random forest model (ranger) was trained using the following environmental predictors: elevation, climate, nitrogen deposition and management intensity, as well as species identity.
Three cross-validation strategies were compared: \
Random CV: a random 5-fold split \
Spatial CV: folds defined by geographic clusters \
Environmental CV: folds defined in temperature–precipitation space. \
The models performance was evaluated using R² and RMSE.

## Results
The results of the model performance are loaded and combined to do a comparison.

```{r}
random <- readRDS(here::here("data/results_random_cv.rds"))
spatial <- readRDS(here::here("data/results_spatial_cv.rds"))
```

```{r}
knitr::include_graphics(here::here("fig/plot.png"))
```
The data is not distributed uniformly over the globe. Especially over Europe and Asia are covered very densly whereas Africa and almost the whole southern hemisphere almost don't exhibit any sampling points.

This uneven distribution means that global spatial upscaling is largely based on relationships learned from temperate, data-rich regions. Therefore, predictions in poorly sampled regions are likely to rely on extrapolation rather than interpolation, which leads to higher uncertainty and potentially biased estimates of leaf nitrogen. In this context, spatial cross-validation is particularly important, as random cross-validation would not reveal these limitations and would overestimate the performance of the global model.
```{r}
knitr::include_graphics(here::here("fig/spatial.png"))
```
The data is unevenly distributed across the five clusters. Cluster 1 covers Eastern Europe; Cluster 2 includes Asia, particularly China; Cluster 3 covers Northern Europe; Cluster 4 covers Central Europe; and Cluster 5 spans North and South America, as well as Western Europe. Most of the data points are concentrated in Clusters 3 and 4 (Northern and Middle Europe) and in China (Cluster 2), while Cluster 5 is more sparsely distributed. This implies that a model will likely perform well in dense clusters (European and Chinese), but poorly in regions with fewer data points. Spatial cross-validation, which involves leaving entire clusters out for testing, reveals how well the model can predict in regions with fewer data points and provides a more realistic assessment of its global transferability.
```{r}
knitr::include_graphics(here::here("fig/spatial_cluster.png"))
```
The boxplot illustrates the variation in leaf nitrogen content within each spatial cluster. The differences in median values and spread between the clusters suggest that leaf N content varies geographically. This supports the need for spatial cross-validation to assess the performance of the model in new regions. The poor spatial CV performance of Cluster 2 (Asia) is explained by its low overlap with the other clusters.
```{r}
environmental <- readRDS(here::here("data/results_env_cv.rds"))
```
```{r}
knitr::include_graphics(here::here("fig/environmental.png"))
```
Points are clustered in environmental space according to mean annual temperature and precipitation. The clusters are mostly separated along the precipitation axis, indicating that precipitation is the dominant differentiating factor. Environmental cross-validation tests assess how well the model generalises across these distinct environmental conditions.
Predictions are most reliable when test points fall within the range of environmental conditions covered by the training data. Extrapolating beyond this range, as occurs with some geographically isolated clusters, increases uncertainty and can result in poor model performance.
```{r}
results <- bind_rows(
random |> mutate(cv_type = "Random"),
spatial |> mutate(cv_type = "Spatial"),
environmental |> mutate(cv_type = "Environmental"))

results |>
group_by(cv_type) |>
summarise(
mean_rsq = mean(rsq, na.rm = TRUE),
mean_rmse = mean(rmse, na.rm = TRUE),
)
```
The visual comparison below shows model performance across validation strategies.
```{r}
knitr::include_graphics(here::here("fig/comparison.png"))
```
## Discussion
Random CV produces a high R² value and a lower RMSE because the data is randomly split. As nearby points are likely to be included in both the training and test sets, the model 'cheats' by predicting locations similar to the training points.
Spatial CV shows lower R² and higher RMSE, because test folds are geographically separate. This tests the model’s ability to predict in new regions, i.e. its spatial generalisation.
These differences highlight spatial overfitting.
Our model may perform well locally CV, but poorly in unobserved regions (spatial CV).
This emphasises the need for careful spatial validation when upscaling ecological data globally.


The results show that model performance varies significantly depending on the chosen cross-validation strategy.\
The environmental CV method omits clusters defined by mean annual temperature and precipitation rather than location.\
The R² and RMSE values are worse than random cross-validation because the model has to predict in unseen environmental conditions, but generally better than the worst spatial CV fold.\
This makes sense, as some environmental combinations are more common across regions, meaning the model can generalise to new areas as long as the environmental values overlap with those in the training set. \
This aligns with the concept of the area of applicability (AOA), whereby predictions are more reliable in regions whose environmental conditions are similar to those of the training data.

